{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3584,"status":"ok","timestamp":1696388984716,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"},"user_tz":-420},"id":"mzmQzCHz9cIy"},"outputs":[],"source":["'''\n","Programmer : Agus Nursikuwagus\n","Create :  January, 3, 2023\n","This program is an implementation of image captioning geological rocks images.\n","Some issues related to other model and tested with baseline model.\n","We called the program is SemAtt Captioning\n","'''\n","\n","import numpy as np\n","from numpy import array\n","\n","import matplotlib.pyplot as plt\n","#%matplotlib inline\n","\n","\n","import string\n","import os\n","import glob\n","import tensorflow as tf\n","\n","from time import time\n","\n","from keras import Input, layers\n","from keras import optimizers\n","\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing import sequence\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import LSTM, Embedding, Dense, Activation, Flatten, Reshape, Dropout\n","from tensorflow.keras.layers import add\n","from tensorflow.keras.applications.xception import Xception\n","from tensorflow.keras.applications.xception import preprocess_input\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.utils import to_categorical"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8_oA-8jyWo3O","executionInfo":{"status":"ok","timestamp":1696389005258,"user_tz":-420,"elapsed":20551,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"}},"outputId":"c7cf06e3-98ab-4e8d-cf90-e270e510360a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1696389045060,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"},"user_tz":-420},"id":"5kz2DP909cI9","outputId":"cdf410fa-cbc7-493e-aca2-4ee044373f89"},"outputs":[{"output_type":"stream","name":"stdout","text":["1.jpg#0 Singkapan batuan sedimen klastik dengan bidang perlapisan yang tidak tegas, batulumpur karbonatan, masif, retak-retak, sebagian hancur dan mulai lapuk\n","1.jpg#1 Singkapan batuan sedimen klastik dengan bidang perlapisan yang tidak tegas, masif, retak-retak, sebagian hancur sehingga mulai lapuk dan batulumpur karbonatan\n","1.jpg#2 Singkapan batuan sedimen klastik dan batulumpur karbonatan \n","1.jpg#3 batulumpur karbonatan dan Singkapan batuan sedimen klastik\n","1.jpg#4 Singkapan batuan sedimen klastik dengan bidang perlapisan yang tidak tegas dan batulumpur karbonatan\n","2.jpg#0 Pecahan koral\n","2.jpg#1 Pecahan koral\n","2.jpg#2 Pecahan koral\n","2.jpg#3 Pecahan koral\n","2.jpg#4 Pecahan koral\n","3.jpg#0 Singkapan Ba\n"]}],"source":["token_path = \"/content/drive/MyDrive/dataset_captioning/Flickr8K_Text/Flickr8k.token.txt\"\n","train_images_path = '/content/drive/MyDrive/dataset_captioning/Flickr8K_Text/Flickr_8k.trainImages.txt'\n","test_images_path = '/content/drive/MyDrive/dataset_captioning/Flickr8K_Text/Flickr_8k.testImages.txt'\n","images_path = '/content/drive/MyDrive/dataset_captioning/Flicker8k_Dataset/'\n","test_path ='/content/drive/MyDrive/dataset_captioning/test_image/'\n","glove_path = '/content/drive/MyDrive/Colab Notebooks/embedding/'\n","\n","doc = open(token_path,'r').read()\n","print(doc[:700])"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1696389051181,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"},"user_tz":-420},"id":"nNuzqGb29cI9"},"outputs":[],"source":["descriptions = dict()\n","for line in doc.split('\\n'):\n","        tokens = line.split()\n","        if len(line) > 2:\n","            image_id = tokens[0].split('.')[0]\n","            image_desc = ' '.join(tokens[1:])\n","            if image_id not in descriptions:\n","                descriptions[image_id] = list()\n","            descriptions[image_id].append(image_desc)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":346,"status":"ok","timestamp":1696389056490,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"},"user_tz":-420},"id":"7RkQsc5Z9cI-"},"outputs":[],"source":["table = str.maketrans('', '', string.punctuation)\n","for key, desc_list in descriptions.items():\n","    for i in range(len(desc_list)):\n","        desc = desc_list[i]\n","        desc = desc.split()\n","        desc = [word.lower() for word in desc]\n","        desc = [w.translate(table) for w in desc]\n","        desc_list[i] =  ' '.join(desc)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1696389057826,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"},"user_tz":-420},"id":"XLXcBmWF9cJH","outputId":"a1716b91-9e2a-4039-e6fe-a1f2c14d324b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original Vocabulary Size: 397\n"]}],"source":["vocabulary = set()\n","for key in descriptions.keys():\n","        [vocabulary.update(d.split()) for d in descriptions[key]]\n","print('Original Vocabulary Size: %d' % len(vocabulary))"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1696389059765,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"},"user_tz":-420},"id":"nBn25i8r9cJH"},"outputs":[],"source":["lines = list()\n","for key, desc_list in descriptions.items():\n","    for desc in desc_list:\n","        lines.append(key + ' ' + desc)\n","\n","new_descriptions = '\\n'.join(lines)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":476,"status":"ok","timestamp":1696389060710,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"},"user_tz":-420},"id":"-Ttvng9h9cJH"},"outputs":[],"source":["doc = open(train_images_path,'r').read()\n","dataset = list()\n","for line in doc.split('\\n'):\n","    if len(line) > 1:\n","      identifier = line.split('.')[0]\n","      dataset.append(identifier)\n","\n","train = set(dataset)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":4217,"status":"ok","timestamp":1696389066903,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"},"user_tz":-420},"id":"WZKZ60qj9cJI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ec85aa2a-d1b8-4892-9919-43e75f269559"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/dataset_captioning/test_image/\n","{'403.jpg', '424.jpg', '556.jpg', '507.jpg', '779.jpg', '568.jpg', '758.jpg', '825.jpg', '335.jpg', '443.jpg', '723.jpg', '720.jpg', '217.jpg', '206.jpg', '241.jpg', '248.jpg', '316.jpg', '505.jpg', '642.jpg', '43.jpg', '110.jpg', '188.jpg', '775.jpg', '810.jpg', '305.jpg', '693.jpg', '699.jpg', '615.jpg', '37.jpg', '609.jpg', '717.jpg', '571.jpg', '603.jpg', '831.jpg', '434.jpg', '665.jpg', '383.jpg', '267.jpg', '117.jpg', '672.jpg', '182.jpg', '196.jpg', '176.jpg', '532.jpg', '11.jpg', '7.jpg', '19.jpg', '394.jpg', '529.jpg', '339.jpg', '539.jpg', '821.jpg', '33.jpg', '650.jpg', '840.jpg', '420.jpg', '518.jpg', '254.jpg', '590.jpg', '812.jpg', '490.jpg', '451.jpg', '156.jpg', '787.jpg', '88.jpg', '848.jpg', '356.jpg', '272.jpg', '413.jpg', '653.jpg', '581.jpg', '679.jpg', '99.jpg', '129.jpg'}\n"]}],"source":["img = glob.glob(images_path + '*.jpg')\n","train_images = set(open(train_images_path, 'r').read().strip().split('\\n'))\n","train_img = []\n","for i in img:\n","    if i[len(images_path):] in train_images:\n","        train_img.append(i)\n","\n","#print(train_img)\n","\n","print(test_path)\n","imgs = glob.glob(test_path + '*.jpg')\n","test_images = set(open(test_images_path, 'r').read().strip().split('\\n'))\n","print(test_images)\n","test_img = []\n","for j in imgs:\n","    if j[len(test_path):] in test_images:\n","        test_img.append(j)\n","\n","#print(test_img)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1696389066904,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"},"user_tz":-420},"id":"KrfZLNUS9cJI"},"outputs":[],"source":["train_descriptions = dict()\n","for line in new_descriptions.split('\\n'):\n","    tokens = line.split()\n","    image_id, image_desc = tokens[0], tokens[1:]\n","    if image_id in train:\n","        if image_id not in train_descriptions:\n","            train_descriptions[image_id] = list()\n","        desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n","        train_descriptions[image_id].append(desc)"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1696389070103,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"},"user_tz":-420},"id":"tXPbwPWi9cJI"},"outputs":[],"source":["all_train_captions = []\n","for key, val in train_descriptions.items():\n","    for cap in val:\n","        all_train_captions.append(cap)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":422,"status":"ok","timestamp":1696389073537,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"},"user_tz":-420},"id":"G4fAI-5G9cJJ","outputId":"d59460b8-2565-4d9b-aad3-79a10ed12293"},"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary = 379\n"]}],"source":["word_count_threshold = 0\n","word_counts = {}\n","nsents = 0\n","for sent in all_train_captions:\n","    nsents += 1\n","    for w in sent.split(' '):\n","        word_counts[w] = word_counts.get(w, 0) + 1\n","vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n","\n","print('Vocabulary = %d' % (len(vocab)))"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1696389077317,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"},"user_tz":-420},"id":"pMZwsYF_9cJJ"},"outputs":[],"source":["ixtoword = {}\n","wordtoix = {}\n","ix = 1\n","for w in vocab:\n","    wordtoix[w] = ix\n","    ixtoword[ix] = w\n","    ix += 1\n","\n","vocab_size = len(ixtoword) + 1"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1696389079825,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"},"user_tz":-420},"id":"lPpT_rQs9cJJ","outputId":"6c2cc0c8-cb67-4712-ba93-3ee7d3e39108"},"outputs":[{"output_type":"stream","name":"stdout","text":["Description Length: 22\n"]}],"source":["all_desc = list()\n","for key in train_descriptions.keys():\n","    [all_desc.append(d) for d in train_descriptions[key]]\n","lines = all_desc\n","max_length = max(len(d.split()) for d in lines)\n","\n","print('Description Length: %d' % max_length)"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":361,"status":"ok","timestamp":1696389082450,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"},"user_tz":-420},"id":"2z0Oe8IH9cJJ"},"outputs":[],"source":["embeddings_index = {}\n","f = open(os.path.join(glove_path, 'geo_glove.txt'), encoding=\"utf-8\")\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1696389086959,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"},"user_tz":-420},"id":"IK_uB3Cv9cJK"},"outputs":[],"source":["embedding_dim = 100\n","embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","for word, i in wordtoix.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":5768,"status":"ok","timestamp":1696389094849,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"},"user_tz":-420},"id":"R5V2ZpV_9cJK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c9c42f8b-2249-492a-c4e3-ad3a817145e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels.h5\n","91884032/91884032 [==============================] - 1s 0us/step\n"]}],"source":["model = tf.keras.applications.xception.Xception()\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":439,"status":"ok","timestamp":1696389097741,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"},"user_tz":-420},"id":"QIae_H1W9cJL"},"outputs":[],"source":["def preprocess(image_path):\n","    img = image.load_img(image_path, target_size=(299, 299))\n","    x = image.img_to_array(img)\n","    x = np.expand_dims(x, axis=0)\n","    x = preprocess_input(x)\n","    return x"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":432,"status":"ok","timestamp":1696389100396,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"},"user_tz":-420},"id":"BK15zVYU9cJN"},"outputs":[],"source":["model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":515,"status":"ok","timestamp":1696389103535,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"},"user_tz":-420},"id":"uKY2Y1859cJN"},"outputs":[],"source":["def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n","    X1, X2, y = list(), list(), list()\n","    n=0\n","    # loop for ever over images\n","    while 1:\n","        for key, desc_list in descriptions.items():\n","            n+=1\n","            # retrieve the photo feature\n","            photo = photos[key+'.jpg']\n","            for desc in desc_list:\n","                # encode the sequence\n","                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n","                # split one sequence into multiple X, y pairs\n","                for i in range(1, len(seq)):\n","                    # split into input and output pair\n","                    in_seq, out_seq = seq[:i], seq[i]\n","                    # pad input sequence\n","                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n","                    # encode output sequence\n","                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n","                    # store\n","                    X1.append(photo)\n","                    X2.append(in_seq)\n","                    y.append(out_seq)\n","\n","            if n==num_photos_per_batch:\n","                yield ([array(X1), array(X2)], array(y))\n","                X1, X2, y = list(), list(), list()\n","                n=0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Og2cVeH9cJO"},"outputs":[],"source":["print(history.history['loss'])\n","print(history.history['accuracy'])\n","\n","model.save('./input/xception_lstm.h5')"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":1830,"status":"error","timestamp":1696389122781,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"},"user_tz":-420},"id":"t_i-ajFY9cJP","colab":{"base_uri":"https://localhost:8080/","height":400},"outputId":"c41f5680-bc8c-443a-c55a-838bd007a81c"},"outputs":[{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-b5eb054d176e>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./input/xception_lstm.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgreedySearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphoto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;31m# Legacy case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     return legacy_sm_saving_lib.load_model(\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/legacy/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                             raise IOError(\n\u001b[0m\u001b[1;32m    235\u001b[0m                                 \u001b[0;34mf\"No file or directory found at {filepath_str}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                             )\n","\u001b[0;31mOSError\u001b[0m: No file or directory found at ./input/xception_lstm.h5"]}],"source":["from nltk.translate.bleu_score import corpus_bleu\n","from keras.models import load_model\n","\n","model = load_model('./input/xception_lstm.h5')\n","\n","def greedySearch(photo):\n","    in_text = 'startseq'\n","    for i in range(max_length):\n","        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n","        sequence = pad_sequences([sequence], maxlen=max_length)\n","        yhat = model.predict([photo,sequence], verbose=0)\n","        yhat = np.argmax(yhat)\n","        word = ixtoword[yhat]\n","        in_text += ' ' + word\n","        if word == 'endseq':\n","            break\n","\n","    final = in_text.split()\n","    final = final[1:-1]\n","    final = ' '.join(final)\n","\n","\n","    return final"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i7QatW3U9cJP"},"outputs":[],"source":["import pandas as pd\n","\n","def beam_search_predictions(image, beam_index = 3):\n","    start = [wordtoix[\"startseq\"]]\n","    start_word = [[start, 0.0]]\n","    i = 0\n","    while len(start_word[0][0]) < max_length:\n","        temp = []\n","        for s in start_word:\n","            par_caps = sequence.pad_sequences([s[0]], maxlen=max_length, padding='post')\n","            #print(par_caps)\n","            #print('\\n')\n","\n","            preds = model.predict([image,par_caps], verbose=0)\n","            #print(preds)\n","            #print('\\n')\n","\n","            word_preds = np.argsort(preds[0])[-beam_index:]\n","            # Getting the top <beam_index>(n) predictions and creating a\n","            # new list so as to put them via the model again\n","            for w in word_preds:\n","\n","                next_cap, prob = s[0][:], s[1]\n","                next_cap.append(w)\n","                prob += preds[0][w]\n","                temp.append([next_cap, prob])\n","                df = pd.DataFrame(temp, columns = ['Sequence Word','Probabilty'])\n","                #print(df.head(10))\n","                #print('\\n')\n","\n","\n","        start_word = temp\n","        # Sorting according to the probabilities\n","        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n","        # Getting the top words\n","        start_word = start_word[-beam_index:]\n","        df_word = pd.DataFrame(start_word)\n","        #print('\\n')\n","\n","\n","    start_word = start_word[-1][0]\n","    intermediate_caption = [ixtoword[i] for i in start_word]\n","    final_caption = []\n","\n","    for i in intermediate_caption:\n","        if i != 'endseq':\n","            final_caption.append(i)\n","        else:\n","            break\n","\n","    final_caption = ' '.join(final_caption[1:])\n","    #print(final_caption)\n","\n","    return final_caption"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1245,"status":"ok","timestamp":1640914245423,"user":{"displayName":"Agus Nursikuwagus","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh6urh3zUCPZtGYUpmBYHjANR48ZvVOvmvhVDHt=s64","userId":"11953329030949391162"},"user_tz":-420},"id":"RcTId7Fj9cJP","outputId":"a381f194-097a-40c0-c306-64f8c6c36860"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\agus2\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import tensorflow as tf\n","import nltk\n","nltk.download('punkt')\n","\n","from nltk import word_tokenize\n","\n","def tokeniser(text_generation):\n","    token = []\n","    #i = 0\n","    for in_txt in text_generation:\n","        refer = word_tokenize(in_txt)\n","        token.append(refer)\n","        #print(token[i])\n","        #i =+ 1\n","\n","\n","    list_token = token\n","    return list_token\n","\n","#print(tokeniser(descriptions['1']))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m-OuKeGuLjNI"},"outputs":[],"source":["from nltk.translate.bleu_score import sentence_bleu\n","\n","\n","for img in list(encoding_train.keys()):\n","    print(img)\n","    picture = plt.imread(images_path + img)\n","    print('nama file = ' + img)\n","    image = encoding_train[img].reshape(1,4096)\n","    plt.imshow(picture)\n","    plt.show()\n","\n","    text = img.split('.')[0]\n","    print(descriptions[text])\n","    references = tokeniser(descriptions[text])\n","    #print('\\n')\n","\n","\n","    print(\"Greedy:\",greedySearch(image))\n","\n","    print(\"Beam Search, K = 3:\",beam_search_predictions(image, beam_index = 3))\n","\n","    candidates = word_tokenize(beam_search_predictions(image, beam_index = 3))\n","    #print(\"\\n\")\n","\n","    #print(\"Beam Search, K = 5:\",beam_search_predictions(image, beam_index = 5))\n","    #print(\"\\n\")\n","\n","    print('BLEU-1: %f' % sentence_bleu(references, candidates, weights=(1.0, 0, 0, 0)))\n","    print('BLEU-2: %f' % sentence_bleu(references, candidates, weights=(0.5, 0.5, 0, 0)))\n","    print('BLEU-3: %f' % sentence_bleu(references, candidates, weights=(0.3, 0.3, 0.3, 0)))\n","    print('BLEU-4: %f' % sentence_bleu(references, candidates, weights=(0.25, 0.25, 0.25, 0.25)))\n","    print(\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4axlt0A48uMH"},"outputs":[],"source":["from nltk.translate.bleu_score import sentence_bleu\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import multilabel_confusion_matrix\n","import seaborn as sns\n","\n","pic = '1.jpg'\n","image = encoding_test[pic].reshape((1,4096))\n","x=plt.imread(test_path + pic)\n","plt.imshow(x)\n","plt.show()\n","\n","print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n","print(\"Greedy Search Predictions :\",greedySearch(image))\n","print('\\n')\n","\n","print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n","print(\"Beam Search Predictions, K = 3:\",beam_search_predictions(image, beam_index = 3))\n","print('\\n')\n","\n","print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n","print(\"Beam Search Predictions, K = 5:\",beam_search_predictions(image, beam_index = 5))\n","print('\\n')\n","\n","\n","references = tokeniser(descriptions['1'])\n","print(references)\n","\n","candidates = word_tokenize(beam_search_predictions(image, beam_index = 3))\n","\n","print(references)\n","#print(candidates)\n","\n","print('\\n')\n","\n","print('BLEU-1: %f' % sentence_bleu(references, candidates, weights=(1.0, 0, 0, 0)))\n","print('BLEU-2: %f' % sentence_bleu(references, candidates, weights=(0.5, 0.5, 0, 0)))\n","print('BLEU-3: %f' % sentence_bleu(references, candidates, weights=(0.3, 0.3, 0.3, 0)))\n","print('BLEU-4: %f' % sentence_bleu(references, candidates, weights=(0.25, 0.25, 0.25, 0.25)))\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i1GRK6L2V4GC"},"outputs":[],"source":["'''\n","import csv\n","with open('./input/incept_lstm_blue.csv','w', newline='') as f:\n","  for ndx in captions:\n","    wr = csv.writer(f, quoting=csv.QUOTE_ALL)\n","    wr.writerow(ndx)\n","\n","\n","import csv\n","with open('./input/xcept_lstm_rouge.csv','w', newline='') as f:\n","  for ndx in rougelist:\n","    wr = csv.writer(f, quoting=csv.QUOTE_ALL)\n","    wr.writerow(ndx)\n","\n","'''\n","\n","import csv\n","with open('./input/xcept_lstm_meteor.csv','w', newline='') as f:\n","  for ndx in meteorlist:\n","    wr = csv.writer(f, quoting=csv.QUOTE_ALL)\n","    wr.writerow(ndx)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GIf3U4ndV4GC"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}