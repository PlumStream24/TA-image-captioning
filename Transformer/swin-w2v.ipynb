{"cells":[{"cell_type":"markdown","metadata":{"id":"SMaRv8tuMeh7"},"source":["# Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"7zbiNa5l8f3i","executionInfo":{"status":"ok","timestamp":1699348194551,"user_tz":-420,"elapsed":10218,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"}}},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","import glob\n","import os\n","import json\n","import time\n","import string\n","import re\n","\n","from torch import nn\n","from torch import Tensor\n","from PIL import Image\n","from tqdm import tqdm\n","\n","import torchvision.transforms as transforms\n","#from torchvision.transforms import Compose, Resize, ToTensor\n","from torchvision.models import swin_t, Swin_T_Weights\n","from torch.nn import TransformerDecoder, TransformerDecoderLayer\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from nltk.translate.bleu_score import corpus_bleu"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"VHuvGzF384mB","executionInfo":{"status":"ok","timestamp":1699348194552,"user_tz":-420,"elapsed":29,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"}}},"outputs":[],"source":["token_path = \"/content/drive/MyDrive/dataset_captioning/Flickr8K_Text/Flickr8k.token.txt\"\n","train_images_path = '/content/drive/MyDrive/dataset_captioning/Flickr8K_Text/Flickr_8k.trainImages.txt'\n","test_images_path = '/content/drive/MyDrive/dataset_captioning/Flickr8K_Text/Flickr_8k.testImages.txt'\n","val_images_path = '/content/drive/MyDrive/dataset_captioning/Flickr8K_Text/Flickr_8k.devImages.txt'\n","\n","images_path = '/content/drive/MyDrive/dataset_captioning/Flicker8k_Dataset/'\n","\n","test_path ='/content/drive/MyDrive/dataset_captioning/test_image/'\n","checkpoint_path = '/content/drive/MyDrive/Colab Notebooks/Checkpoints/'\n","run_path = '/content/drive/MyDrive/Colab Notebooks/runs/'"]},{"cell_type":"markdown","metadata":{"id":"DpM4Gw663FkI"},"source":["# Class Declaration"]},{"cell_type":"markdown","metadata":{"id":"5rZlq_qKWRs7"},"source":["## Model"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"3PcdVrjFWD6F","executionInfo":{"status":"ok","timestamp":1699348194552,"user_tz":-420,"elapsed":25,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"}}},"outputs":[],"source":["class EncoderSwin(nn.Module):\n","    def __init__(self, embed_size = 100):\n","        super(EncoderSwin, self).__init__()\n","\n","        swin = swin_t(Swin_T_Weights.IMAGENET1K_V1)\n","        self.swin = torch.nn.Sequential(*(list(swin.children())[:-1]))\n","\n","    def forward(self, images):\n","\n","        img_features = self.swin(images)\n","\n","        return img_features\n","\n","class ResidualBlock(nn.Module):\n","    \"\"\"Represents 1D version of the residual block: https://arxiv.org/abs/1512.03385\"\"\"\n","\n","    def __init__(self, input_dim):\n","        \"\"\"Initializes the module.\"\"\"\n","        super(ResidualBlock, self).__init__()\n","        self.block = nn.Sequential(\n","            nn.Linear(input_dim, input_dim),\n","            nn.LeakyReLU(),\n","            nn.Linear(input_dim, input_dim),\n","        )\n","\n","    def forward(self, x):\n","        \"\"\"Performs forward pass of the module.\"\"\"\n","        skip_connection = x\n","        x = self.block(x)\n","        x = skip_connection + x\n","        return x\n","\n","\n","class Normalize(nn.Module):\n","    def __init__(self, eps=1e-5):\n","        super(Normalize, self).__init__()\n","        self.register_buffer(\"eps\", torch.Tensor([eps]))\n","\n","    def forward(self, x, dim=-1):\n","        norm = x.norm(2, dim=dim).unsqueeze(-1)\n","        x = self.eps * (x / norm)\n","        return x\n","\n","class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        position = torch.arange(max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n","        pe = torch.zeros(max_len, 1, d_model)\n","        pe[:, 0, 0::2] = torch.sin(position * div_term)\n","        pe[:, 0, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        \"\"\"\n","        Arguments:\n","            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n","        \"\"\"\n","        x = x + self.pe[:x.size(0)]\n","        return self.dropout(x)\n","\n","class CaptionDecoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        ## Configs ##\n","        decoder_layers = 6\n","        attention_heads = 16\n","        d_model = 512\n","        ff_dim = 1024\n","        dropout = 0.5\n","        embedding_dim = 100\n","        img_feature_dim = 768\n","        vocab_size = 401\n","        embedding_path = '/content/drive/MyDrive/Colab Notebooks/embedding/w2v-embeddings.txt'\n","\n","        ## Embeddings ##\n","        word_embeddings = torch.Tensor(np.loadtxt(embedding_path))\n","        self.embedding_layer = nn.Embedding.from_pretrained(\n","            word_embeddings,\n","            freeze=True,\n","            padding_idx=0\n","        )\n","\n","        ## Layers ##\n","        self.entry_mapping_words = nn.Linear(embedding_dim, d_model)\n","        self.entry_mapping_img = nn.Linear(img_feature_dim, d_model)\n","\n","        self.res_block = ResidualBlock(d_model)\n","\n","        self.positional_encoding = PositionalEncoding(d_model=d_model, dropout=dropout, max_len = 64)\n","        dec_layer = TransformerDecoderLayer(\n","            d_model=d_model,\n","            nhead=attention_heads,\n","            dim_feedforward=ff_dim,\n","            dropout=dropout\n","        )\n","        self.decoder = TransformerDecoder(dec_layer, num_layers = decoder_layers)\n","        self.classifier = nn.Linear(d_model, vocab_size)\n","\n","    def forward(self, x, image_features, tgt_padding_mask=None, tgt_mask=None):\n","        ## Process Image ##\n","        image_features = self.entry_mapping_img(image_features)\n","        if (image_features.dim() == 2):\n","            image_features = image_features.unsqueeze(0)\n","        #image_features = image_features.permute(1,0,2)\n","        image_features = F.leaky_relu(image_features)\n","\n","        ## Process Caption ##\n","        # Embedding\n","        #with torch.no_grad():\n","        #    outputs = self.emb_model(x)\n","        #x = outputs.last_hidden_state\n","        #x = torch.squeeze(x, dim=0) #This one stays disabled\n","\n","        x = self.embedding_layer(x)\n","        x = self.entry_mapping_words(x)\n","        x = F.leaky_relu(x)\n","\n","        x = self.res_block(x)\n","        x = F.leaky_relu(x)\n","\n","        x = x.permute(1,0,2)\n","        x = self.positional_encoding(x)\n","\n","        ## Decode Image and Caption ##\n","        x = self.decoder(\n","            tgt=x,\n","            memory=image_features,\n","            tgt_key_padding_mask=tgt_padding_mask,\n","            tgt_mask=tgt_mask\n","        )\n","\n","        x = x.permute(1,0,2)\n","\n","        x = self.classifier(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"T0gTbabZWUHp"},"source":["## Dataloader"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"HSf1pzXmEBIt","executionInfo":{"status":"ok","timestamp":1699348194553,"user_tz":-420,"elapsed":24,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"}}},"outputs":[],"source":["class Flickr8KDataset(Dataset):\n","    def __init__(self, path_list, training=True):\n","        # Read tokens, split lines\n","        with open(path_list) as g:\n","            train_list = [line.replace(\"\\n\", \"\") for line in g.readlines()]\n","        with open(token_path, \"r\") as f:\n","            self._data = []\n","            for line in f.readlines() :\n","                if (line.split(\"#\")[0] in train_list) :\n","                    self._data.append(line.replace(\"\\n\",\"\"))\n","\n","        self._training = training\n","        self._inference_captions = self._group_captions(self._data)\n","\n","        # Tokens\n","        self._pad_idx = 0\n","        self._start_idx = 1\n","        self._end_idx = 2\n","        self._unk_idx = 3\n","        self._pad_token = '<pad>'\n","        self._start_token = '<start>'\n","        self._end_token = '<end>'\n","        self._unk_token = '<unk>'\n","\n","        # Load the vocabulary mappings\n","        word2idx_path = '/content/drive/MyDrive/Colab Notebooks/embedding/w2v-word2idx.json'\n","        with open(word2idx_path, \"r\", encoding=\"utf8\") as f:\n","            self._word2idx = json.load(f)\n","        self._idx2word = {str(idx): word for word, idx in self._word2idx.items()}\n","\n","        # Create (X,Y) pairs\n","        self._data = self._create_input_label_mappings(self._data)\n","\n","        self.image_dir = images_path\n","\n","        # For image preprocessing\n","        self._preproc = self._construct_image_transform(224)\n","\n","        self._max_len = 32\n","        self._dataset_size = len(self._data)\n","\n","    def _construct_image_transform(self, image_size):\n","        normalize = transforms.Normalize(\n","            mean=[0.485, 0.456, 0.406],\n","            std=[0.229, 0.224, 0.225]\n","        )\n","        preprocessing = transforms.Compose([\n","            transforms.Resize(image_size),\n","            transforms.CenterCrop(image_size),\n","            transforms.ToTensor(),\n","            normalize,\n","        ])\n","\n","        return preprocessing\n","\n","    def _create_input_label_mappings(self, data):\n","        # Creates (image, description) pairs.\n","        processed_data = []\n","\n","        for line in data:\n","            tokens = line.split()\n","            # Seperate image and caption\n","            img_name, caption_words = tokens[0].split(\"#\")[0], tokens[1:]\n","\n","            pair = (img_name, caption_words)\n","            processed_data.append(pair)\n","\n","        return processed_data\n","\n","    def _load_and_prepare_image(self, image_name):\n","        # Image preprocessing\n","        image_path = os.path.join(self.image_dir, image_name)\n","        img_pil = Image.open(image_path).convert(\"RGB\")\n","        image_tensor = self._preproc(img_pil)\n","        #image_tensor = image_tensor.unsqueeze(0)\n","        return image_tensor\n","\n","    def _group_captions(self, data):\n","        table = str.maketrans('', '', string.punctuation)\n","        grouped_captions = {}\n","\n","        for line in data:\n","            tokens = line.split()\n","            if len(line) > 2:\n","                image_id, image_desc = tokens[0].split('#')[0], tokens[1:]\n","\n","                image_desc = [token.strip().lower().translate(table) for token in image_desc]\n","\n","                if image_id not in grouped_captions:\n","                    grouped_captions[image_id] = []\n","                grouped_captions[image_id].append(image_desc)\n","\n","        return grouped_captions\n","\n","    def _load_and_process_images(self, image_dir, image_names):\n","        image_paths = [os.path.join(image_dir, fname) for fname in image_names]\n","        image_raws = [Image.open(path) for path in image_paths]\n","\n","        image_tensors = [self._preproc(img) for img in image_raws]\n","        #image_tensors = [img.unsqueeze(0) for img in image_tensors]\n","\n","        image_processed = {img_name: img_tensor for img_name, img_tensor in zip(image_names, image_tensors)}\n","\n","        return image_processed\n","\n","    def inference_batch(self, batch_size):\n","        caption_data_items = list(self._inference_captions.items())\n","\n","        num_batches = len(caption_data_items) // batch_size\n","        for idx in range(num_batches):\n","            caption_samples = caption_data_items[idx * batch_size: (idx + 1) * batch_size]\n","            batch_imgs = []\n","            batch_captions = []\n","\n","            # Increase index for the next batch\n","            idx += batch_size\n","\n","            # Create a mini batch data\n","            for image_name, captions in caption_samples:\n","                batch_captions.append(captions)\n","                batch_imgs.append(self._load_and_prepare_image(image_name))\n","\n","            # Batch image tensors\n","            batch_imgs = torch.stack(batch_imgs, dim=0)\n","            #if batch_size == 1:\n","            #    batch_imgs = batch_imgs.unsqueeze(0)\n","\n","            yield batch_imgs, batch_captions\n","\n","    def __len__(self):\n","        return self._dataset_size\n","\n","    def __getitem__(self, index):\n","        table = str.maketrans('', '', string.punctuation)\n","\n","        image_id, tokens = self._data[index]\n","\n","        # Load and preprocess image\n","        image_tensor = self._load_and_prepare_image(image_id)\n","        # preprocess caption and add tokens\n","        tokens = [token.strip().lower().translate(table) for token in tokens]\n","        tokens = [self._start_token] + tokens + [self._end_token]\n","\n","        # Create input and target tokens\n","        input_tokens = tokens[:-1].copy()\n","        tgt_tokens = tokens[1:].copy()\n","\n","        # previously disabled\n","        sample_size = len(input_tokens)\n","        padding_size = self._max_len - sample_size\n","\n","        if padding_size > 0:\n","            padding_vec = [self._pad_token for _ in range(padding_size)]\n","            input_tokens += padding_vec.copy()\n","            tgt_tokens += padding_vec.copy()\n","\n","        input_tokens = [self._word2idx.get(token, self._unk_idx) for token in input_tokens]\n","        tgt_tokens = [self._word2idx.get(token, self._unk_idx) for token in tgt_tokens]\n","\n","        # Tokens to Tensor\n","        input_tokens = torch.tensor(input_tokens).long()\n","        tgt_tokens = torch.tensor(tgt_tokens).long()\n","\n","        # Create padding masks for captions\n","        tgt_padding_mask = torch.ones([self._max_len, ])\n","        tgt_padding_mask[:sample_size] = 0.0\n","        tgt_padding_mask = tgt_padding_mask.bool()\n","\n","        return image_tensor, input_tokens, tgt_tokens, tgt_padding_mask\n","\n"]},{"cell_type":"markdown","metadata":{"id":"62Aie7eBWZDr"},"source":["## Utils"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"eS8HQou0WbEh","executionInfo":{"status":"ok","timestamp":1699348194553,"user_tz":-420,"elapsed":23,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"}}},"outputs":[],"source":["def set_up_causal_mask(seq_len, device):\n","    \"\"\"Defines the triangular mask used in transformers.\n","        This mask prevents decoder from attending the tokens after the current one.\n","    \"\"\"\n","    mask = (torch.triu(torch.ones(seq_len, seq_len)) == 1).transpose(0, 1)\n","    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).to(device)\n","    mask.requires_grad = False\n","    return mask\n","\n","def log_gradient_norm(model, writer, step, mode, norm_type=2):\n","    \"\"\"Writes model param's gradients norm to tensorboard\"\"\"\n","    total_norm = 0\n","    for p in model.parameters():\n","        if p.requires_grad:\n","            param_norm = p.grad.data.norm(norm_type)\n","            total_norm += param_norm.item() ** 2\n","    total_norm = total_norm ** (1. / 2)\n","    writer.add_scalar(f\"Gradient/{mode}\", total_norm, step)\n","\n","def save_checkpoint(name, encoder, decoder, enc_optimizer, dec_optimizer, start_time, epoch):\n","    \"\"\"Saves specified model checkpoint.\"\"\"\n","    #target_dir = os.path.join(checkpoint_path, str(start_time) + f'_{name}')\n","    target_dir = str(start_time) + f'_{name}'\n","    os.makedirs(target_dir, exist_ok=True)\n","\n","    PATH = os.path.join(target_dir, f\"{name}_{epoch}.pth\")\n","\n","    torch.save({\n","            'epoch': epoch,\n","            'encoder': encoder.state_dict(),\n","            'decoder': decoder.state_dict(),\n","            'enc_optimizer': enc_optimizer.state_dict(),\n","            'dec_optimizer': dec_optimizer.state_dict(),\n","    }, PATH)\n","\n","    print(\"Model saved.\")"]},{"cell_type":"markdown","metadata":{"id":"hHw_S9sA6_QL"},"source":["## Evaluate"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"64wLJoRn7BZ0","executionInfo":{"status":"ok","timestamp":1699348194554,"user_tz":-420,"elapsed":22,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"}}},"outputs":[],"source":["def inference(decoder, img_features, start_idx, end_idx, pad_idx, idx2word, batch_size, max_len, device):\n","    # Input words [<start>, <pad>, ...] + padding mask [False, ..., True]\n","    x_words = torch.Tensor([start_idx] + [pad_idx] * (max_len - 1)).to(device).long()\n","    x_words = x_words.repeat(batch_size, 1)\n","    padd_mask = torch.Tensor([True] * max_len).to(device).bool()\n","    padd_mask = padd_mask.repeat(batch_size, 1)\n","\n","    # Flag for each image\n","    is_decoded = [False] * batch_size\n","    generated_captions = []\n","    for _ in range(batch_size):\n","        generated_captions.append([])\n","\n","    for i in range(max_len - 1):\n","        # Update padding masks\n","        padd_mask[:, i] = False\n","\n","        # Prediction for next word\n","        y_pred_prob = decoder(x_words, img_features, padd_mask)\n","        y_pred_prob = y_pred_prob[torch.arange(batch_size), [i] * batch_size].clone()\n","        y_pred = y_pred_prob.argmax(-1)\n","\n","        # Add the generated word to generated_captions\n","        for batch_idx in range(batch_size):\n","            if is_decoded[batch_idx]:\n","                continue\n","            generated_captions[batch_idx].append(idx2word[str(y_pred[batch_idx].item())])\n","            if y_pred[batch_idx] == end_idx:\n","                is_decoded[batch_idx] = True\n","\n","        if np.all(is_decoded):\n","            break\n","\n","        if i < (max_len - 1):\n","            # Update the input tokens for the next iteration\n","            x_words[torch.arange(batch_size), [i+1] * batch_size] = y_pred.view(-1)\n","\n","    # Add end token to unfinished caption\n","    for batch_idx in range(batch_size):\n","        if not is_decoded[batch_idx]:\n","            generated_captions[batch_idx].append(idx2word[str(end_idx)])\n","\n","    # Clean the EOS symbol\n","    for caption in generated_captions:\n","        caption.remove(\"<end>\")\n","\n","    return generated_captions\n","\n","\n","\n","def evaluate(dataset, encoder, decoder, device):\n","    batch_size = 4\n","    max_len = 32\n","    bleu_w = {\n","        \"bleu-1\": [1.0],\n","        \"bleu-2\": [0.5, 0.5],\n","        \"bleu-3\": [0.333, 0.333, 0.333],\n","        \"bleu-4\": [0.25, 0.25, 0.25, 0.25]\n","    }\n","\n","    idx2word = dataset._idx2word\n","    start_idx = dataset._start_idx\n","    end_idx = dataset._end_idx\n","    pad_idx = dataset._pad_idx\n","\n","    references = []\n","    predictions = []\n","\n","    for x_img, y_caption in dataset.inference_batch(batch_size):\n","        x_img = x_img.to(device)\n","\n","        # Extract image features\n","        with torch.no_grad():\n","            img_features = encoder(x_img)\n","\n","        pred_captions = inference(decoder, img_features, start_idx, end_idx, pad_idx, idx2word, batch_size, max_len, device)\n","        references += y_caption\n","        predictions += pred_captions\n","\n","    # Evaluate BLEU\n","    bleu_1 = corpus_bleu(references, predictions, weights=bleu_w[\"bleu-1\"]) * 100\n","    bleu_2 = corpus_bleu(references, predictions, weights=bleu_w[\"bleu-2\"]) * 100\n","    bleu_3 = corpus_bleu(references, predictions, weights=bleu_w[\"bleu-3\"]) * 100\n","    bleu_4 = corpus_bleu(references, predictions, weights=bleu_w[\"bleu-4\"]) * 100\n","    bleu = [bleu_1, bleu_2, bleu_3, bleu_4]\n","\n","    return bleu\n"]},{"cell_type":"markdown","metadata":{"id":"VgVEt-e4WWrG"},"source":["## Trainer"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"GxDPaXB-8ELH","executionInfo":{"status":"ok","timestamp":1699348194554,"user_tz":-420,"elapsed":20,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"}}},"outputs":[],"source":["def train(device, writer, model_name, checkpoint=None) :\n","\n","    torch.manual_seed(2023)\n","    np.random.seed(2023)\n","\n","    ## Encoder ##\n","    encoder = EncoderSwin()\n","    encoder = encoder.to(device)\n","    if checkpoint!=None:\n","        encoder.load_state_dict(checkpoint['encoder'])\n","    encoder.train()\n","\n","    ## Decoder ##\n","    decoder = CaptionDecoder()\n","    decoder = decoder.to(device)\n","    if checkpoint!=None:\n","        decoder.load_state_dict(checkpoint['decoder'])\n","    decoder.train()\n","\n","    ## Config ##\n","    train_config = {\n","        \"epochs\": 50,\n","        \"warmup_steps\": 0,\n","        \"learning_rate\": 5e-6,\n","        \"l2_penalty\": 1e-2,\n","        \"gradient_clipping\": 2.0,\n","        \"save_period\": 5,\n","        \"eval_period\": 5\n","    }\n","    train_hyperparams = {\n","        \"batch_size\" : 4,\n","        \"shuffle\" : True\n","    }\n","    early_stopping = 10\n","    epochs_since_improvement = 0\n","    min_loss = 100\n","\n","    # Create dataloader\n","    train_set = Flickr8KDataset(train_images_path, training=True)\n","    val_set = Flickr8KDataset(val_images_path, training=False)\n","    train_loader = DataLoader(train_set, **train_hyperparams)\n","    val_loader = DataLoader(val_set, **train_hyperparams)\n","\n","    causal_mask = set_up_causal_mask(32, device)\n","\n","    # Optimizer\n","    enc_optimizer = torch.optim.AdamW(\n","        encoder.parameters(),\n","        lr = train_config[\"learning_rate\"],\n","        weight_decay = train_config[\"l2_penalty\"]\n","    )\n","    dec_optimizer = torch.optim.AdamW(\n","        decoder.parameters(),\n","        lr = train_config[\"learning_rate\"],\n","        weight_decay = train_config[\"l2_penalty\"]\n","    )\n","    if checkpoint!=None:\n","        enc_optimizer.load_state_dict(checkpoint['enc_optimizer'])\n","        dec_optimizer.load_state_dict(checkpoint['dec_optimizer'])\n","\n","    # LR Scheduler\n","    #enc_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(enc_optimizer, 'min')\n","    #dec_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(dec_optimizer, 'min')\n","\n","    # Loss function\n","    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1, ignore_index=0)\n","\n","    ## Start Train ##\n","    start_time = time.strftime(\"%b-%d_%H-%M-%S\")\n","    train_step = 0\n","\n","    # Load epoch checkpoint\n","    if checkpoint!=None:\n","        load_epoch = checkpoint['epoch']\n","    else:\n","        load_epoch = 0\n","\n","    for epoch in range(load_epoch+1, train_config[\"epochs\"]+1):\n","\n","        encoder.train()\n","        decoder.train()\n","        train_loss = []\n","        val_loss = []\n","\n","        with tqdm(train_loader) as tepoch:\n","            for x_img, x_words, y, tgt_padding_mask in tepoch:\n","                tepoch.set_description(f\"Epoch {epoch}\")\n","\n","                enc_optimizer.zero_grad()\n","                dec_optimizer.zero_grad()\n","                train_step += 1\n","\n","                # Move tensor to device\n","                x_img, x_words = x_img.to(device), x_words.to(device)\n","                y = y.to(device)\n","                tgt_padding_mask = tgt_padding_mask.to(device)\n","\n","                # Extract image features\n","                img_features = encoder(x_img)\n","\n","                # Prediction from decoder\n","                y_pred = decoder(x_words, img_features, tgt_padding_mask, causal_mask)\n","                tgt_padding_mask = torch.logical_not(tgt_padding_mask)\n","                y_pred = y_pred[tgt_padding_mask]\n","\n","                y = y[tgt_padding_mask]\n","\n","                # Calculate loss\n","                loss = loss_fn(y_pred, y.long())\n","                # Backpropagation\n","                loss.backward()\n","\n","                # Log gradient\n","                torch.nn.utils.clip_grad_norm_(encoder.parameters(), train_config[\"gradient_clipping\"])\n","                torch.nn.utils.clip_grad_norm_(decoder.parameters(), train_config[\"gradient_clipping\"])\n","\n","                # Update weights\n","                enc_optimizer.step()\n","                dec_optimizer.step()\n","\n","                # Log loss\n","                train_loss.append(loss.item())\n","                avg_train_loss = sum(train_loss) / len(train_loss)\n","                tepoch.set_postfix(loss=avg_train_loss)\n","\n","            writer.add_scalar(\"Train/Epoch-Loss\", avg_train_loss, epoch)\n","\n","        # Validation\n","        encoder.eval()\n","        decoder.eval()\n","\n","        with tqdm(val_loader) as vepoch:\n","\n","            for x_img, x_words, y, tgt_padding_mask in vepoch:\n","                vepoch.set_description(f\"Validation \")\n","                # Move tensor to device\n","                x_img, x_words = x_img.to(device), x_words.to(device)\n","                y = y.to(device)\n","                tgt_padding_mask = tgt_padding_mask.to(device)\n","\n","                # Extract image features\n","                with torch.no_grad():\n","                    # Extract image features\n","                    img_features = encoder(x_img)\n","\n","                    # Prediction from decoder\n","                    y_pred = decoder(x_words, img_features, tgt_padding_mask, causal_mask)\n","                    tgt_padding_mask = torch.logical_not(tgt_padding_mask)\n","                    y_pred = y_pred[tgt_padding_mask]\n","\n","                    y = y[tgt_padding_mask]\n","\n","                    # Calculate loss\n","                    loss = loss_fn(y_pred, y.long())\n","\n","                val_loss.append(loss.item())\n","                avg_val_loss = sum(val_loss) / len(val_loss)\n","                vepoch.set_postfix(loss=avg_val_loss)\n","\n","            writer.add_scalar(\"Valid/Epoch-Loss\", avg_val_loss, epoch)\n","\n","        #enc_scheduler.step(avg_val_epoch_loss)\n","        #dec_scheduler.step(avg_val_epoch_loss)\n","\n","        # Early Stopping\n","        '''\n","        if min_loss <= avg_val_epoch_loss:\n","            epochs_since_improvement += 1\n","            print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n","        else:\n","            epochs_since_improvement = 0\n","            min_loss = min(min_loss, avg_val_epoch_loss)\n","        if epochs_since_improvement == early_stopping:\n","            save_checkpoint(model_name, encoder, decoder, enc_optimizer, dec_optimizer, start_time, epoch)\n","            break\n","        '''\n","\n","\n","        # Save model state\n","        if (epoch) % train_config['save_period'] == 0:\n","            save_checkpoint(model_name, encoder, decoder, enc_optimizer, dec_optimizer, start_time, epoch)\n","\n","        # Evaluate model performance\n","        if (epoch) % train_config[\"eval_period\"] == 0:\n","            with torch.no_grad():\n","                encoder.eval()\n","                decoder.eval()\n","\n","                # Evaluate model performance on subsets\n","                train_bleu = evaluate(train_set, encoder, decoder, device)\n","                val_bleu = evaluate(val_set, encoder, decoder, device)\n","\n","                print('Train BLEU', train_bleu)\n","                print('Valid BLEU', val_bleu)\n","\n","                # Log the evaluated BLEU score\n","                for i, tr_b in enumerate(train_bleu):\n","                    writer.add_scalar(f\"Train/BLEU-{i+1}\", tr_b, epoch)\n","                for i, tv_b in enumerate(val_bleu):\n","                    writer.add_scalar(f\"Valid/BLEU-{i+1}\", tv_b, epoch)\n","\n","                encoder.train()\n","                decoder.train()\n","\n","    writer.flush()"]},{"cell_type":"markdown","metadata":{"id":"hUJDt6n9WcAw"},"source":["# Main"]},{"cell_type":"markdown","metadata":{"id":"MtUST8C2TJ9V"},"source":["## Tensorboard"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"HHmzNoxKeSgZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699355906086,"user_tz":-420,"elapsed":347,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"}},"outputId":"afba4ae5-a6b4-46ff-df0a-853e9a5e3022"},"outputs":[{"output_type":"stream","name":"stdout","text":["The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n"]}],"source":["%load_ext tensorboard"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":191},"executionInfo":{"elapsed":3930,"status":"ok","timestamp":1699355911705,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"},"user_tz":-420},"id":"RO4nmp5runG3","outputId":"12bd09c7-82e7-4226-84b9-b43e42800655"},"outputs":[{"output_type":"display_data","data":{"text/plain":["ERROR: Failed to launch TensorBoard (exited with 1).\n","Contents of stderr:\n","2023-11-07 11:18:27.972868: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-11-07 11:18:27.972943: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-11-07 11:18:27.972974: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-11-07 11:18:28.985377: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Error: A logdir or db must be specified. For example `tensorboard --logdir mylogdir` or `tensorboard --db sqlite:~/.tensorboard.db`. Run `tensorboard --helpfull` for details and examples."]},"metadata":{}}],"source":["#%tensorboard --logdir='/content/drive/MyDrive/Colab Notebooks/runs/'\n","%tensorboard"]},{"cell_type":"code","source":["!kill"],"metadata":{"id":"NBveohJkgPZr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vaBCsrkQoJHi"},"source":["## Train"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pGMff0bGH6KT","outputId":"f1b57e55-570a-4dec-d4a7-ba58aa79dd8f","executionInfo":{"status":"ok","timestamp":1699355800550,"user_tz":-420,"elapsed":7588740,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Running on cuda\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n","  warnings.warn(\n","Downloading: \"https://download.pytorch.org/models/swin_t-704ceda3.pth\" to /root/.cache/torch/hub/checkpoints/swin_t-704ceda3.pth\n","100%|██████████| 108M/108M [00:00<00:00, 141MB/s] \n","Epoch 1:   0%|          | 0/871 [00:05<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","Epoch 1: 100%|██████████| 871/871 [06:36<00:00,  2.20it/s, loss=4.85]\n","Validation : 100%|██████████| 93/93 [00:32<00:00,  2.87it/s, loss=4.58]\n","Epoch 2: 100%|██████████| 871/871 [02:15<00:00,  6.41it/s, loss=4.36]\n","Validation : 100%|██████████| 93/93 [00:09<00:00,  9.66it/s, loss=4.43]\n","Epoch 3: 100%|██████████| 871/871 [02:15<00:00,  6.44it/s, loss=4.18]\n","Validation : 100%|██████████| 93/93 [00:07<00:00, 12.16it/s, loss=4.23]\n","Epoch 4: 100%|██████████| 871/871 [02:15<00:00,  6.44it/s, loss=4]\n","Validation : 100%|██████████| 93/93 [00:07<00:00, 13.00it/s, loss=4.17]\n","Epoch 5: 100%|██████████| 871/871 [02:17<00:00,  6.33it/s, loss=3.81]\n","Validation : 100%|██████████| 93/93 [00:06<00:00, 14.40it/s, loss=4.06]\n"]},{"output_type":"stream","name":"stdout","text":["Model saved.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 2-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 3-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 4-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n"]},{"output_type":"stream","name":"stdout","text":["Train BLEU [22.158504937426805, 4.088006633307689e-153, 3.732489607799368e-204, 5.552609865984194e-230]\n","Valid BLEU [20.155851277058407, 4.0337558982386097e-153, 3.784419724859241e-204, 5.70642150360679e-230]\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6: 100%|██████████| 871/871 [02:15<00:00,  6.43it/s, loss=3.65]\n","Validation : 100%|██████████| 93/93 [00:07<00:00, 12.74it/s, loss=3.99]\n","Epoch 7: 100%|██████████| 871/871 [02:16<00:00,  6.40it/s, loss=3.54]\n","Validation : 100%|██████████| 93/93 [00:08<00:00, 11.18it/s, loss=3.94]\n","Epoch 8: 100%|██████████| 871/871 [02:16<00:00,  6.40it/s, loss=3.44]\n","Validation : 100%|██████████| 93/93 [00:06<00:00, 13.35it/s, loss=3.82]\n","Epoch 9: 100%|██████████| 871/871 [02:15<00:00,  6.42it/s, loss=3.38]\n","Validation : 100%|██████████| 93/93 [00:07<00:00, 12.36it/s, loss=3.83]\n","Epoch 10: 100%|██████████| 871/871 [02:16<00:00,  6.39it/s, loss=3.3]\n","Validation : 100%|██████████| 93/93 [00:07<00:00, 12.30it/s, loss=3.82]\n"]},{"output_type":"stream","name":"stdout","text":["Model saved.\n","Train BLEU [29.265367426576148, 4.7050794422899815e-153, 4.102936082337701e-204, 5.96586539553077e-230]\n","Valid BLEU [22.17143640476425, 4.2306388774311403e-153, 3.90645712016477e-204, 5.844024377529853e-230]\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 11: 100%|██████████| 871/871 [02:16<00:00,  6.37it/s, loss=3.24]\n","Validation : 100%|██████████| 93/93 [00:06<00:00, 13.76it/s, loss=3.78]\n","Epoch 12: 100%|██████████| 871/871 [02:16<00:00,  6.38it/s, loss=3.18]\n","Validation : 100%|██████████| 93/93 [00:07<00:00, 12.68it/s, loss=3.75]\n","Epoch 13: 100%|██████████| 871/871 [02:17<00:00,  6.35it/s, loss=3.14]\n","Validation : 100%|██████████| 93/93 [00:07<00:00, 12.31it/s, loss=3.79]\n","Epoch 14: 100%|██████████| 871/871 [02:14<00:00,  6.45it/s, loss=3.09]\n","Validation : 100%|██████████| 93/93 [00:07<00:00, 12.11it/s, loss=3.75]\n","Epoch 15: 100%|██████████| 871/871 [02:16<00:00,  6.39it/s, loss=3.04]\n","Validation : 100%|██████████| 93/93 [00:06<00:00, 13.58it/s, loss=3.74]\n"]},{"output_type":"stream","name":"stdout","text":["Model saved.\n","Train BLEU [32.290096900067475, 3.11276287426979, 2.483260705680467e-102, 1.5526929932839672e-153]\n","Valid BLEU [21.163643840911327, 4.1333698091149876e-153, 3.8464078648728565e-204, 5.776452046050107e-230]\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 16: 100%|██████████| 871/871 [02:14<00:00,  6.48it/s, loss=3.01]\n","Validation : 100%|██████████| 93/93 [00:06<00:00, 14.11it/s, loss=3.73]\n","Epoch 17: 100%|██████████| 871/871 [02:14<00:00,  6.49it/s, loss=2.99]\n","Validation : 100%|██████████| 93/93 [00:07<00:00, 13.08it/s, loss=3.77]\n","Epoch 18: 100%|██████████| 871/871 [02:15<00:00,  6.44it/s, loss=2.94]\n","Validation : 100%|██████████| 93/93 [00:07<00:00, 12.98it/s, loss=3.83]\n","Epoch 19: 100%|██████████| 871/871 [02:14<00:00,  6.50it/s, loss=2.92]\n","Validation : 100%|██████████| 93/93 [00:07<00:00, 12.47it/s, loss=3.78]\n","Epoch 20: 100%|██████████| 871/871 [02:14<00:00,  6.49it/s, loss=2.89]\n","Validation : 100%|██████████| 93/93 [00:07<00:00, 12.41it/s, loss=3.86]\n"]},{"output_type":"stream","name":"stdout","text":["Model saved.\n","Train BLEU [36.10310968115946, 5.23934383360648, 3.61238611553975e-102, 2.100717395267331e-153]\n","Valid BLEU [21.667540122837785, 4.1822871308732315e-153, 3.876665381271931e-204, 5.810532887640725e-230]\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 21: 100%|██████████| 871/871 [02:13<00:00,  6.52it/s, loss=2.86]\n","Validation : 100%|██████████| 93/93 [00:06<00:00, 15.02it/s, loss=3.85]\n","Epoch 22: 100%|██████████| 871/871 [02:13<00:00,  6.52it/s, loss=2.83]\n","Validation : 100%|██████████| 93/93 [00:06<00:00, 14.68it/s, loss=3.87]\n","Epoch 23: 100%|██████████| 871/871 [02:15<00:00,  6.42it/s, loss=2.81]\n","Validation : 100%|██████████| 93/93 [00:06<00:00, 14.82it/s, loss=3.88]\n","Epoch 24: 100%|██████████| 871/871 [02:13<00:00,  6.53it/s, loss=2.78]\n","Validation : 100%|██████████| 93/93 [00:06<00:00, 14.92it/s, loss=3.82]\n","Epoch 25: 100%|██████████| 871/871 [02:15<00:00,  6.44it/s, loss=2.76]\n","Validation : 100%|██████████| 93/93 [00:06<00:00, 14.25it/s, loss=3.87]\n"]},{"output_type":"stream","name":"stdout","text":["Model saved.\n","Train BLEU [48.61283011050161, 15.140226964724226, 8.068093626394063e-102, 4.12800298454336e-153]\n","Valid BLEU [24.042089922115874, 4.781709294114566e-153, 4.4768431948084944e-204, 6.74355098927916e-230]\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 26: 100%|██████████| 871/871 [02:13<00:00,  6.55it/s, loss=2.75]\n","Validation : 100%|██████████| 93/93 [00:07<00:00, 12.49it/s, loss=3.87]\n","Epoch 27: 100%|██████████| 871/871 [02:12<00:00,  6.55it/s, loss=2.72]\n","Validation : 100%|██████████| 93/93 [00:07<00:00, 12.38it/s, loss=3.95]\n","Epoch 28: 100%|██████████| 871/871 [02:14<00:00,  6.47it/s, loss=2.7]\n","Validation : 100%|██████████| 93/93 [00:07<00:00, 12.63it/s, loss=3.9]\n","Epoch 29: 100%|██████████| 871/871 [02:12<00:00,  6.55it/s, loss=2.69]\n","Validation : 100%|██████████| 93/93 [00:07<00:00, 12.53it/s, loss=3.99]\n","Epoch 30: 100%|██████████| 871/871 [02:12<00:00,  6.57it/s, loss=2.66]\n","Validation : 100%|██████████| 93/93 [00:08<00:00, 10.63it/s, loss=3.93]\n"]},{"output_type":"stream","name":"stdout","text":["Model saved.\n","Train BLEU [65.54101334839211, 30.599774674705056, 4.496492543228723, 1.0788245068637742e-76]\n","Valid BLEU [31.025835362637018, 8.117415247118956, 5.340524387248961e-102, 3.034189765567951e-153]\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 31: 100%|██████████| 871/871 [02:13<00:00,  6.53it/s, loss=2.64]\n","Validation : 100%|██████████| 93/93 [00:06<00:00, 14.19it/s, loss=3.98]\n","Epoch 32: 100%|██████████| 871/871 [02:13<00:00,  6.53it/s, loss=2.63]\n","Validation : 100%|██████████| 93/93 [00:06<00:00, 14.05it/s, loss=4.02]\n","Epoch 33: 100%|██████████| 871/871 [02:14<00:00,  6.48it/s, loss=2.61]\n","Validation : 100%|██████████| 93/93 [00:07<00:00, 12.90it/s, loss=4.01]\n","Epoch 34: 100%|██████████| 871/871 [02:11<00:00,  6.60it/s, loss=2.59]\n","Validation : 100%|██████████| 93/93 [00:07<00:00, 13.20it/s, loss=4]\n","Epoch 35: 100%|██████████| 871/871 [02:12<00:00,  6.57it/s, loss=2.58]\n","Validation : 100%|██████████| 93/93 [00:07<00:00, 13.13it/s, loss=4.01]\n"]},{"output_type":"stream","name":"stdout","text":["Model saved.\n","Train BLEU [77.32727106196523, 52.391821202711306, 12.372719477663216, 2.4151942851845872e-76]\n","Valid BLEU [38.39884507789223, 21.575470528003958, 1.1331117735197271e-101, 5.755694466947957e-153]\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 36: 100%|██████████| 871/871 [02:13<00:00,  6.54it/s, loss=2.56]\n","Validation : 100%|██████████| 93/93 [00:06<00:00, 13.50it/s, loss=4.02]\n","Epoch 37: 100%|██████████| 871/871 [02:13<00:00,  6.55it/s, loss=2.56]\n","Validation : 100%|██████████| 93/93 [00:06<00:00, 13.74it/s, loss=4.02]\n","Epoch 38: 100%|██████████| 871/871 [02:14<00:00,  6.46it/s, loss=2.54]\n","Validation : 100%|██████████| 93/93 [00:07<00:00, 12.89it/s, loss=4.04]\n","Epoch 39: 100%|██████████| 871/871 [02:12<00:00,  6.58it/s, loss=2.53]\n","Validation : 100%|██████████| 93/93 [00:07<00:00, 12.99it/s, loss=4.12]\n","Epoch 40: 100%|██████████| 871/871 [02:12<00:00,  6.58it/s, loss=2.51]\n","Validation : 100%|██████████| 93/93 [00:06<00:00, 13.44it/s, loss=4.04]\n"]},{"output_type":"stream","name":"stdout","text":["Model saved.\n","Train BLEU [81.18963308150377, 58.75509740412117, 29.034590695537588, 4.6713455612081645e-76]\n","Valid BLEU [40.328478885324415, 22.395831644399866, 8.215735686880324, 1.7643260223123518e-76]\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 41: 100%|██████████| 871/871 [02:14<00:00,  6.49it/s, loss=2.49]\n","Validation : 100%|██████████| 93/93 [00:06<00:00, 15.11it/s, loss=4.06]\n","Epoch 42: 100%|██████████| 871/871 [02:12<00:00,  6.56it/s, loss=2.48]\n","Validation : 100%|██████████| 93/93 [00:06<00:00, 14.86it/s, loss=4.08]\n","Epoch 43: 100%|██████████| 871/871 [02:14<00:00,  6.48it/s, loss=2.48]\n","Validation : 100%|██████████| 93/93 [00:06<00:00, 14.96it/s, loss=4.1]\n","Epoch 44: 100%|██████████| 871/871 [02:12<00:00,  6.57it/s, loss=2.47]\n","Validation : 100%|██████████| 93/93 [00:06<00:00, 14.71it/s, loss=4.12]\n","Epoch 45: 100%|██████████| 871/871 [02:12<00:00,  6.59it/s, loss=2.45]\n","Validation : 100%|██████████| 93/93 [00:06<00:00, 15.00it/s, loss=4.09]\n"]},{"output_type":"stream","name":"stdout","text":["Model saved.\n","Train BLEU [81.43551568073697, 62.42729856066337, 37.79219909991227, 5.677670222389189e-76]\n","Valid BLEU [37.60362611045187, 22.21795536269732, 11.767079879738889, 2.3083422177148206e-76]\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 46: 100%|██████████| 871/871 [02:17<00:00,  6.36it/s, loss=2.43]\n","Validation : 100%|██████████| 93/93 [00:07<00:00, 11.98it/s, loss=4.15]\n","Epoch 47: 100%|██████████| 871/871 [02:17<00:00,  6.32it/s, loss=2.41]\n","Validation : 100%|██████████| 93/93 [00:06<00:00, 13.99it/s, loss=4.12]\n","Epoch 48: 100%|██████████| 871/871 [02:16<00:00,  6.37it/s, loss=2.41]\n","Validation : 100%|██████████| 93/93 [00:06<00:00, 14.80it/s, loss=4.18]\n","Epoch 49: 100%|██████████| 871/871 [02:17<00:00,  6.32it/s, loss=2.41]\n","Validation : 100%|██████████| 93/93 [00:07<00:00, 12.50it/s, loss=4.1]\n","Epoch 50: 100%|██████████| 871/871 [02:13<00:00,  6.53it/s, loss=2.39]\n","Validation : 100%|██████████| 93/93 [00:07<00:00, 12.48it/s, loss=4.12]\n"]},{"output_type":"stream","name":"stdout","text":["Model saved.\n","Train BLEU [82.3556458995109, 66.5217156362642, 43.87715391160611, 6.379897961538142e-76]\n","Valid BLEU [40.52766428452431, 24.872957734923993, 14.174125227944568, 2.693863712590783e-76]\n"]}],"source":["start_time = time.strftime(\"%b-%d_%H-%M-%S\")\n","model_name = 'swin-trans-lr56-wd2'\n","run_dir = os.path.join(run_path, str(start_time) + f'_{model_name}')\n","\n","writer = SummaryWriter()\n","use_gpu = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n","print(\"Running on\", device)\n","\n","#PATH = '/content/Nov-04_04-06-34_swin-trans-finetune/swin-trans-finetune.pth'\n","#checkpoint = torch.load(PATH)\n","\n","train(device, writer, model_name)"]},{"cell_type":"code","source":["import shutil\n","shutil.copy(\"/content/Nov-07_09-10-25_swin-trans-lr56-wd2/swin-trans-lr56-wd2_25.pth\", \"/content/drive/MyDrive/\")\n","#shutil.copy('/content/runs/Nov04_04-06-13_435e95dac81f/events.out.tfevents.1699070773.435e95dac81f.249.0', \"/content/drive/MyDrive/\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"9HF-1azG5X8P","executionInfo":{"status":"ok","timestamp":1699355831594,"user_tz":-420,"elapsed":5950,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"}},"outputId":"d712242e-80d5-4ea5-92a3-103ba8c1719c"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/swin-trans-lr56-wd2_25.pth'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"PWP6hUrHd48f"},"source":["# Evaluate Test"]},{"cell_type":"markdown","metadata":{"id":"U6-Bf8MH8qwK"},"source":["## Inference Test Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z0yOwFVTgb_4","executionInfo":{"status":"aborted","timestamp":1699348200931,"user_tz":-420,"elapsed":22,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"}}},"outputs":[],"source":["def inference_test(decoder, img_features, start_idx, end_idx, pad_idx, idx2word, batch_size, max_len, device):\n","    # Input words [<start>, <pad>, ...] + padding mask [False, ..., True]\n","    x_words = torch.Tensor([start_idx] + [pad_idx] * (max_len - 1)).to(device).long()\n","    x_words = x_words.repeat(batch_size, 1)\n","    padd_mask = torch.Tensor([True] * max_len).to(device).bool()\n","    padd_mask = padd_mask.repeat(batch_size, 1)\n","\n","    # Flag for each image\n","    is_decoded = [False] * batch_size\n","    generated_captions = []\n","    for _ in range(batch_size):\n","        generated_captions.append([])\n","\n","    for i in range(max_len - 1):\n","        # Update padding masks\n","        padd_mask[:, i] = False\n","\n","        # Prediction for next word\n","        y_pred_prob = decoder(x_words, img_features, padd_mask)\n","        y_pred_prob = y_pred_prob[torch.arange(batch_size), [i] * batch_size].clone()\n","        y_pred = y_pred_prob.argmax(-1)\n","\n","        # Add the generated word to generated_captions\n","        for batch_idx in range(batch_size):\n","            if is_decoded[batch_idx]:\n","                continue\n","            generated_captions[batch_idx].append(idx2word[str(y_pred[batch_idx].item())])\n","            if y_pred[batch_idx] == end_idx:\n","                is_decoded[batch_idx] = True\n","\n","        if np.all(is_decoded):\n","            break\n","\n","        if i < (max_len - 1):\n","            # Update the input tokens for the next iteration\n","            x_words[torch.arange(batch_size), [i+1] * batch_size] = y_pred.view(-1)\n","\n","    # Add end token to unfinished caption\n","    for batch_idx in range(batch_size):\n","        if not is_decoded[batch_idx]:\n","            generated_captions[batch_idx].append(idx2word[str(end_idx)])\n","\n","    # Clean the EOS symbol\n","    for caption in generated_captions:\n","        caption.remove(\"<end>\")\n","\n","    return generated_captions\n","\n","\n","\n","def evaluate_test(dataset, encoder, decoder, device):\n","    batch_size = 4\n","    max_len = 64\n","    bleu_w = {\n","        \"bleu-1\": [1.0],\n","        \"bleu-2\": [0.5, 0.5],\n","        \"bleu-3\": [0.333, 0.333, 0.333],\n","        \"bleu-4\": [0.25, 0.25, 0.25, 0.25]\n","    }\n","\n","    idx2word = dataset._idx2word\n","    start_idx = dataset._start_idx\n","    end_idx = dataset._end_idx\n","    pad_idx = dataset._pad_idx\n","\n","    references = []\n","    predictions = []\n","\n","    print(\"Evaluating...\")\n","    for x_img, y_caption in dataset.inference_batch(batch_size):\n","        x_img = x_img.to(device)\n","\n","        # Extract image features\n","        with torch.no_grad():\n","            img_features = encoder._process_input(x_img)\n","            batch_class_token = encoder.class_token.expand(img_features.shape[0], -1, -1)\n","            img_features = torch.cat([batch_class_token, img_features], dim=1)\n","            img_features = encoder.encoder(img_features)\n","            img_features = img_features[:, 0]\n","            #img_features = img_features.unsqueeze(0)\n","\n","        pred_captions = inference_test(decoder, img_features, start_idx, end_idx, pad_idx, idx2word, batch_size, max_len, device)\n","        references += y_caption\n","        predictions += pred_captions\n","\n","    # Evaluate BLEU\n","    bleu_1 = corpus_bleu(references, predictions, weights=bleu_w[\"bleu-1\"]) * 100\n","    bleu_2 = corpus_bleu(references, predictions, weights=bleu_w[\"bleu-2\"]) * 100\n","    bleu_3 = corpus_bleu(references, predictions, weights=bleu_w[\"bleu-3\"]) * 100\n","    bleu_4 = corpus_bleu(references, predictions, weights=bleu_w[\"bleu-4\"]) * 100\n","    bleu = [bleu_1, bleu_2, bleu_3, bleu_4]\n","\n","    return bleu"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EnfAbsNr9Sam","executionInfo":{"status":"aborted","timestamp":1699348200931,"user_tz":-420,"elapsed":21,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import itertools\n","\n","def caption_test(dataset, encoder, decoder, device, index):\n","    batch_size = 1\n","    max_len = 64\n","\n","    idx2word = dataset._idx2word\n","    start_idx = dataset._start_idx\n","    end_idx = dataset._end_idx\n","    pad_idx = dataset._pad_idx\n","\n","    x_img, y_caption = next(itertools.islice(dataset.inference_batch(batch_size), index, None))\n","\n","    im_prev = x_img.squeeze().permute(1, 2, 0).float()\n","    plt.imshow(im_prev)\n","    plt.show()\n","    x_img = x_img.to(device)\n","\n","    # Extract image features\n","    with torch.no_grad():\n","        img_features = encoder._process_input(x_img)\n","        batch_class_token = encoder.class_token.expand(img_features.shape[0], -1, -1)\n","        img_features = torch.cat([batch_class_token, img_features], dim=1)\n","        img_features = encoder.encoder(img_features)\n","        img_features = img_features[:, 0]\n","        #img_features = img_features.unsqueeze(0)\n","\n","    pred_captions = inference_test(decoder, img_features, start_idx, end_idx, pad_idx, idx2word, batch_size, max_len, device)\n","\n","    print('Reference:')\n","    for ref in y_caption[0]:\n","        print(ref)\n","    print('Prediction: ', pred_captions)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hluYblw5d9Mv","executionInfo":{"status":"aborted","timestamp":1699348200931,"user_tz":-420,"elapsed":21,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"}}},"outputs":[],"source":["device = torch.device(\"cuda\")\n","test_set = Flickr8KDataset(test_images_path, training=False)\n","cp_path = '/content/drive/MyDrive/Colab Notebooks/Checkpoints/Sep-15_05-45-51_pretrained-vit-w2v/model_90.pth'\n","checkpoint = torch.load(cp_path)\n","\n","encoder = vit_b_16()\n","encoder = encoder.to(device)\n","encoder.load_state_dict(checkpoint['encoder'])\n","for name, param in encoder.named_parameters():\n","    if re.match('^heads', name) : param.requires_grad = False\n","encoder.eval()\n","\n","decoder = CaptionDecoder()\n","decoder = decoder.to(device)\n","decoder.load_state_dict(checkpoint['decoder'])\n","decoder.eval()\n"]},{"cell_type":"markdown","metadata":{"id":"7KNTWWgE8xvs"},"source":["## Result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9n9Z8kjBe8t3","executionInfo":{"status":"aborted","timestamp":1699348200932,"user_tz":-420,"elapsed":21,"user":{"displayName":"13519152 Muhammad Iqbal Sigid","userId":"10419761124844586549"}}},"outputs":[],"source":["#test_bleu = evaluate_test(test_set, encoder, decoder, device)\n","#print(test_bleu)\n","\n","caption_test(test_set, encoder, decoder, device, 6)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["hHw_S9sA6_QL","MtUST8C2TJ9V","U6-Bf8MH8qwK"],"provenance":[],"gpuType":"T4","mount_file_id":"1u56bUXWdha3jM1uoHEbKlLISihCza0bm","authorship_tag":"ABX9TyP5pWwvJ24PTx8cDw8MyqJB"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}